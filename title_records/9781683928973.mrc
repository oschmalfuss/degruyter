04030nam a22006375i 45000010014000000030009000140050017000230060019000400070015000590080041000740200018001150240031001330350021001640400033001850410008002180440012002260500025002380720023002630820015002861000081003012450096003822640062004782640011005403000031005513360026005823370026006083380036006343470024006705050502006945060104011965201159013005300026024595380049024855460016025345880112025506500048026626500029027106500032027396500052027716500026028236500034028496530144028837760024030277760025030518560042030768560049031188560080031679120022032479120014032699120015032839120023032989120016033219120016033379120015033539120024033689781683928973DE-B159720240306010644.0m|||||o||d||||||||cr || ||||||||240306t20232024xxu    fo  d z      eng d  a97816839289737 a10.1515/97816839289732doi  a(DE-B1597)658521  aDE-B1597bengcDE-B1597erda0 aeng  axxucUS 4aQA76.9.N38bC36 2023 7aCOM0000002bisacsh04a007.132231 aCampesato, Oswald, eauthor.4aut4http://id.loc.gov/vocabulary/relators/aut10aTransformer, BERT, and GPT :bIncluding ChatGPT and Prompt Engineering /cOswald Campesato. 1aDulles, VA : bMercury Learning and Information, c[2023] 4cÂ©2024  a1 online resource (364 p.)  atextbtxt2rdacontent  acomputerbc2rdamedia  aonline resourcebcr2rdacarrier  atext filebPDF2rda00tFrontmatter -- tContents -- tPreface -- tChapter 1: Introduction -- tChapter 2: Tokenization -- tChapter 3: Transformer Architecture Introduction -- tChapter 4: Transformer Architecture in Greater Depth -- tChapter 5: The BERT Family Introduction -- tChapter 6: The BERT Family in Greater Depth -- tChapter 7: Working with GPT-3 Introduction -- tChapter 8: Working with GPT-3 in Greater Depth -- tChapter 9: ChatGPT and GPT-4 -- tChapter 10: Visualization with Generative AI -- tIndex0 arestricted accessuhttp://purl.org/coar/access_right/c_16ecfonline access with authorization2star  aThis book provides a comprehensive group of topics covering the details of the Transformer architecture, BERT models, and the GPT series, including GPT-3 and GPT-4. Spanning across ten chapters, it begins with foundational concepts such as the attention mechanism, then tokenization techniques, explores the nuances of Transformer and BERT architectures, and culminates in advanced topics related to the latest in the GPT series, including ChatGPT. Key chapters provide insights into the evolution and significance of attention in deep learning, the intricacies of the Transformer architecture, a two-part exploration of the BERT family, and hands-on guidance on working with GPT-3. The concluding chapters present an overview of ChatGPT, GPT-4, and visualization using generative AI. In addition to the primary topics, the book also covers influential AI organizations such as DeepMind, OpenAI, Cohere, Hugging Face, and more. Readers will gain a comprehensive understanding of the current landscape of NLP models, their underlying architectures, and practical applications. Features companion files with numerous code samples and figures from the book.  aIssued also in print.  aMode of access: Internet via World Wide Web.  aIn English.0 aDescription based on online resource; title from PDF title page (publisher's Web site, viewed 06. Mrz 2024) 0aArtificial intelligencexComputer programs. 0aArtificial intelligence. 0aHuman-computer interaction. 0aNatural language processing (Computer science). 0aSoftware engineering. 7aCOMPUTERS / General.2bisacsh  anatural language processing, expert systems, visualization, AI, artificial intelligence, deep learning, machine learning, computer science.0 cEPUBz97816839289660 cprintz978168392898040uhttps://doi.org/10.1515/978168392897340uhttps://www.degruyter.com/isbn/9781683928973423Coveruhttps://www.degruyter.com/document/cover/isbn/9781683928973/original  aEBA_CL_CHCOMSGSEN  aEBA_DGALL  aEBA_EBKALL  aEBA_ECL_CHCOMSGSEN  aEBA_EEBKALL  aEBA_ESTMALL  aEBA_STMALL  aGBV-deGruyter-alles