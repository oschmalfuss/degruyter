05843nam a22004215i 45000010014000000030009000140050017000230060019000400070015000590080041000740200018001150350021001330400033001540410008001870440011001952450042002062640040002482640011002883000031002993360026003303370026003563380036003823470024004185051416004425060104018585202934019625380049048965460015049455880112049607730095050728560049051678560080052169120060052969120015053569120014053719120024053859120012054099782759831463DE-B159720230529101353.0m|||||o||d||||||||cr || ||||||||230529t20232023fr     fo  d z      fre d  a9782759831463  a(DE-B1597)653134  aDE-B1597bengcDE-B1597erda0 afre  afrcFR00aRégression avec R :b3ème édition. 1aLes Ulis : bEDP Sciences, c[2023] 4c©2023  a1 online resource (452 p.)  atextbtxt2rdacontent  acomputerbc2rdamedia  aonline resourcebcr2rdacarrier  atext filebPDF2rda00tFrontmatter -- tREMERCIEMENTS -- tAVANT-PROPOS -- tTable des matières -- tPremière partie Introduction au modèle linéaire -- tChapitre 1 La régression linéaire simple -- tChapitre 2 La régression linéaire multiple -- tChapitre 3 Validation du modèle -- tChapitre 4 Extensions : non-inversibilité et (ou) erreurs corrélées -- tDeuxième partie Inférence -- tChapitre 5 Inférence dans le modèle gaussien -- tChapitre 6 Variables qualitatives : ANCOVA et ANOVA -- tTroisième partie Réduction de dimension -- tChapitre 7 Choix de variables -- tChapitre 8 Régularisation des moindres carrés : ridge, lasso, elastic-net -- tChapitre 9 Régression sur composantes : PCR et PLS -- tChapitre 10 Comparaison des différentes méthodes, étude de cas réels -- tQuatrième partie Le modèle linéaire généralisé -- tChapitre 11 Régression logistique -- tChapitre 12 Régression de Poisson -- tChapitre 13 Régularisation de la vraisemblance -- tChapitre 14 Comparaison des différentes méthodes en classification supervisée, études de cas réel -- tChapitre 15 Données déséquilibrées -- tCinquième partie Introduction à la régression non paramétrique -- tChapitre 16 Introduction à la régression spline -- tChapitre 17 Estimateurs à noyau et k plus proches voisins -- tAnnexe A Rappels -- tBibliographie -- tIndex -- tNotations -- tFonctions et packages R0 arestricted accessuhttp://purl.org/coar/access_right/c_16ecfonline access with authorization2star  aCet ouvrage expose de manière détaillée, exemples à l’appui, différentes façons de répondre à un des problèmes statistiques les plus courants : la régression. Cette nouvelle édition se décompose en cinq parties. La première donne les grands principes des régressions simple et multiple par moindres carrés. Les fondamentaux de la méthode, tant au niveau des choix opérés que des hypothèses et leur utilité, sont expliqués. La deuxième partie est consacrée à l’inférence et présente les outils permettant de vérifier les hypothèses mises en œuvre. Les techniques d’analyse de la variance et de la covariance sont également présentées dans cette partie. Le cas de la grande dimension est ensuite abordé dans la troisième partie. Différentes méthodes de réduction de la dimension telles que la sélection de variables, les régressions sous contraintes (lasso, elasticnet ou ridge) et sur composantes (PLS ou PCR) sont notamment proposées. Un dernier chapitre propose des algorithmes, basés sur des méthodes de rééchantillonnage comme l’apprentissage/validation ou la validation croisée, qui permettent d’établir une comparaison entre toutes ces méthodes. La quatrième partie se concentre sur les modèles linéaires généralisés et plus particulièrement sur les régressions logistique et de Poisson avec ou sans technique de régularisation. Une section particulière est consacrée aux comparaisons de méthodes en classification supervisée. Elle introduit notamment des critères de performance pour scorer des individus comme les courbes ROC et lift et propose des stratégies de choix seuil (Younden, macro F1.) pour les classer. Ces notions sont ensuite mises en œuvre sur des données réelles afin de sélectionner une méthode de prévision parmi plusieurs algorithmes basés sur des modèles logistiques (régularisés ou non). Une dernière section aborde le problème des données déséquilibrées qui est souvent rencontré en régression binaire. Enfin, la dernière partie présente l’approche non paramétrique à travers les splines, les estimateurs à noyau et des plus proches voisins. La présentation témoigne d’un réel souci pédagogique des auteurs qui bénéficient d’une expérience d’enseignement auprès de publics très variés. Les résultats exposés sont replacés dans la perspective de leur utilité pratique grâce à l’analyse d’exemples concrets. Les commandes permettant le traitement des exemples sous R figurent dans le corps du texte. Enfin, chaque chapitre est complété par une suite d’exercices corrigés. Les codes, les données et les corrections des exercices se trouvent sur le site https://regression-avec-r.github.io/ Cet ouvrage s’adresse principalement à des étudiants de Master et d’écoles d’ingénieurs ainsi qu’aux chercheurs travaillant dans les divers domaines des sciences appliquées  aMode of access: Internet via World Wide Web.  aIn French.0 aDescription based on online resource; title from PDF title page (publisher's Web site, viewed 29. Mai 2023)08iTitle is part of eBook package:dDe GruytertDG Plus PP Package 2023 Part 2z978311117804240uhttps://www.degruyter.com/isbn/9782759831463423Coveruhttps://www.degruyter.com/document/cover/isbn/9782759831463/original  a978-3-11-117804-2  DG Plus PP Package 2023 Part 2b2023  aEBA_EBKALL  aEBA_PPALL  aGBV-deGruyter-alles  aPDA5EBK